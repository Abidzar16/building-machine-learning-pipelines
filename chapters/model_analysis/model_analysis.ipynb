{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing your model with TensorFlow Model Analysis and the What-If Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# stop tf warnings going everywhere\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add project to the python path\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_EVAL_DATA_FILE = 'data_tfrecord-00000-of-00001'\n",
    "_MODEL_DIR = 'serving_model_dir_2000_steps/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the model into the required format for TFMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_shared_model = tfma.default_eval_shared_model(\n",
    "    eval_saved_model_path=_MODEL_DIR, tags=[tf.saved_model.SERVING])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = [tfma.slicer.SingleSliceSpec(),\n",
    "          tfma.slicer.SingleSliceSpec(columns=['product'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config=tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key='consumer_disputed')],\n",
    "        slicing_specs=[tfma.SlicingSpec(), tfma.SlicingSpec(feature_keys=['product'])],\n",
    "        metrics_specs=[\n",
    "              tfma.MetricsSpec(metrics=[\n",
    "                  tfma.MetricConfig(class_name='BinaryAccuracy'),\n",
    "                  tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                  tfma.MetricConfig(class_name='FalsePositives'),\n",
    "                  tfma.MetricConfig(class_name='TruePositives'),\n",
    "                  tfma.MetricConfig(class_name='FalseNegatives'),\n",
    "                  tfma.MetricConfig(class_name='TrueNegatives')\n",
    "              ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config_plot=tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key='consumer_disputed')],\n",
    "        slicing_specs=[tfma.SlicingSpec(), tfma.SlicingSpec(feature_keys=['product'])],\n",
    "    metrics_specs=tfma.metrics.specs_from_metrics([\n",
    "        tfma.metrics.ConfusionMatrixPlot(),\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a minute\n",
    "eval_result = tfma.run_model_analysis(\n",
    "    eval_shared_model=eval_shared_model,\n",
    "    eval_config=eval_config_plot,\n",
    "    data_location=_EVAL_DATA_FILE,\n",
    "    output_path=\"./eval_result_plot\",\n",
    "    file_format='tfrecords',\n",
    "    slice_spec = slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfma.view.render_slicing_metrics(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfma.view.render_slicing_metrics(eval_result, slicing_spec=slices[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "tfma.view.render_plot(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a custom metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom Keras metric\n",
    "# https://github.com/tensorflow/model-analysis/blob/master/g3doc/metrics.md\n",
    "class MyMetric(tf.keras.metrics.Mean):\n",
    "\n",
    "  def __init__(self, name='my_metric', dtype=None):\n",
    "    super(MyMetric, self).__init__(name=name, dtype=dtype)\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    return super(MyMetric, self).update_state(\n",
    "        y_pred, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/model-analysis/blob/master/g3doc/post_export_metrics.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_shared_model_2 = tfma.default_eval_shared_model(\n",
    "    eval_saved_model_path='serving_model_dir_150_steps/', tags=[tf.saved_model.SERVING])\n",
    "\n",
    "eval_result = tfma.run_model_analysis(\n",
    "    eval_shared_model=eval_shared_model_2,\n",
    "    eval_config=eval_config,\n",
    "    data_location=_EVAL_DATA_FILE,\n",
    "    output_path=\"./eval_result_150_steps\",\n",
    "    file_format='tfrecords',\n",
    "    slice_spec = slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results_from_disk = tfma.load_eval_results(\n",
    "    ['./eval_result_2000_steps','./eval_result_150_steps'], tfma.constants.MODEL_CENTRIC_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfma.view.render_time_series(eval_results_from_disk, slices[0])\n",
    "# many js errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/tensorboard/blob/master/docs/fairness-indicators.md\n",
    "!pip install tensorboard_plugin_fairness_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_callbacks = \\\n",
    "    [tfma.post_export_metrics.fairness_indicators(thresholds=[0.25, 0.5, 0.75])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_shared_model_fairness = tfma.default_eval_shared_model(\n",
    "    eval_saved_model_path=_MODEL_DIR,\n",
    "    add_metrics_callbacks=metrics_callbacks,\n",
    "     tags=[tf.saved_model.SERVING])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config_fairness=tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key='consumer_disputed')],\n",
    "        slicing_specs=[tfma.SlicingSpec(), tfma.SlicingSpec(feature_keys=['product'])],\n",
    "        metrics_specs=[\n",
    "              tfma.MetricsSpec(metrics=[\n",
    "                  tfma.MetricConfig(class_name='BinaryAccuracy'),\n",
    "                  tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                  tfma.MetricConfig(class_name='FalsePositives'),\n",
    "                  tfma.MetricConfig(class_name='TruePositives'),\n",
    "                  tfma.MetricConfig(class_name='FalseNegatives'),\n",
    "                  tfma.MetricConfig(class_name='TrueNegatives'),\n",
    "                  tfma.MetricConfig(class_name='FairnessIndicators', config='{\"thresholds\":[0.25, 0.5, 0.75]}')\n",
    "              ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Tensorflow version (2.1.0) found. Note that TFMA support for TF 2.0 is currently in beta\n",
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "WARNING:root:Deleting 1 existing files in target path matching: \n"
     ]
    }
   ],
   "source": [
    "eval_result = tfma.run_model_analysis(\n",
    "    eval_shared_model=eval_shared_model_fairness,\n",
    "    eval_config=eval_config_fairness,\n",
    "    data_location=_EVAL_DATA_FILE,\n",
    "    output_path=\"./eval_result_fairness\",\n",
    "    file_format='tfrecords',\n",
    "    slice_spec = slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard_plugin_fairness_indicators import summary_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.create_file_writer('./fairness_indicator_logs')\n",
    "with writer.as_default():\n",
    "    summary_v2.FairnessIndicators('./eval_result_fairness', step=1)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 58921), started 2:04:32 ago. (Use '!kill 58921' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3905c4af52b45f8e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3905c4af52b45f8e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./fairness_indicator_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The What-If Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from witwidget.notebook.visualization import WitConfigBuilder\n",
    "from witwidget.notebook.visualization import WitWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = tf.data.TFRecordDataset(_EVAL_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_examples = [tf.train.Example.FromString(d.numpy()) for d in eval_data.take(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(export_dir=_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(examples):\n",
    "    preds = model.signatures['serving_default'](examples=tf.constant([example.SerializeToString() for example in examples]))\n",
    "    return preds['outputs'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder = WitConfigBuilder(eval_examples).set_custom_predict_fn(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "WitWidget(config_builder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works with 2.1\n",
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works with >0.21.3\n",
    "!pip show tensorflow_model_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works with 1.6.0\n",
    "!pip show witwidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may need to run this every time\n",
    "!jupyter nbextension install --py --symlink --sys-prefix witwidget\n",
    "\n",
    "!jupyter nbextension enable witwidget --py --sys-prefix \n",
    "\n",
    "# then refresh browser page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may need to run this every time\n",
    "\n",
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "  \n",
    "!jupyter nbextension install --py --symlink tensorflow_model_analysis --sys-prefix\n",
    "  \n",
    "!jupyter nbextension enable --py tensorflow_model_analysis --sys-prefix\n",
    "\n",
    "# then refresh browser page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter serverextension list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
