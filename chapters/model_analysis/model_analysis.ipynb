{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing your model with TensorFlow Model Analysis and the What-If Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# stop tf warnings going everywhere\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add project to the python path\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_EVAL_DATA_FILE = 'data_tfrecord-00000-of-00001'\n",
    "_MODEL_DIR = 'serving_model_dir_2000_steps/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_shared_model = tfma.default_eval_shared_model(\n",
    "    eval_saved_model_path=_MODEL_DIR, tags=[tf.saved_model.SERVING])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = [tfma.slicer.SingleSliceSpec(),\n",
    "          tfma.slicer.SingleSliceSpec(columns=['product'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config=tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key='consumer_disputed')],\n",
    "        slicing_specs=[tfma.SlicingSpec(), tfma.SlicingSpec(feature_keys=['product'])],\n",
    "        metrics_specs=[\n",
    "              tfma.MetricsSpec(metrics=[\n",
    "                  tfma.MetricConfig(class_name='BinaryAccuracy'),\n",
    "                  tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                  tfma.MetricConfig(class_name='FalsePositives'),\n",
    "                  tfma.MetricConfig(class_name='TruePositives'),\n",
    "                  tfma.MetricConfig(class_name='FalseNegatives'),\n",
    "                  tfma.MetricConfig(class_name='TrueNegatives')\n",
    "              ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config_viz=tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key='consumer_disputed')],\n",
    "        slicing_specs=[tfma.SlicingSpec(), tfma.SlicingSpec(feature_keys=['product'])],\n",
    "        metrics_specs=[\n",
    "              tfma.MetricsSpec(metrics=[\n",
    "                  tfma.MetricConfig(class_name='BinaryAccuracy'),\n",
    "                  tfma.MetricConfig(class_name='AUC'),\n",
    "                  tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                  tfma.MetricConfig(class_name='Precision'),\n",
    "                  tfma.MetricConfig(class_name='Recall')\n",
    "              ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfma.view.render_slicing_metrics(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tfma.view.render_slicing_metrics(eval_result, slicing_spec=slices[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config_plot=tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key='consumer_disputed')],\n",
    "        slicing_specs=[tfma.SlicingSpec(), tfma.SlicingSpec(feature_keys=['product'])],\n",
    "    metrics_specs=tfma.metrics.specs_from_metrics([\n",
    "        tfma.metrics.ConfusionMatrixPlot(),\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_result = tfma.run_model_analysis(\n",
    "    eval_shared_model=eval_shared_model,\n",
    "    eval_config=eval_config_plot,\n",
    "    data_location=_EVAL_DATA_FILE,\n",
    "    output_path=\"./eval_result_viz\",\n",
    "    file_format='tfrecords',\n",
    "    slice_spec = slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not work right now, javascript errors\n",
    "tfma.view.render_plot(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a custom metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to do F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom Keras metric\n",
    "# https://github.com/tensorflow/model-analysis/blob/master/g3doc/metrics.md\n",
    "class MyMetric(tf.keras.metrics.Mean):\n",
    "\n",
    "  def __init__(self, name='my_metric', dtype=None):\n",
    "    super(MyMetric, self).__init__(name=name, dtype=dtype)\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    return super(MyMetric, self).update_state(\n",
    "        y_pred, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/model-analysis/blob/master/g3doc/post_export_metrics.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_shared_model_2 = tfma.default_eval_shared_model(\n",
    "    eval_saved_model_path='serving_model_dir_150_steps/', tags=[tf.saved_model.SERVING])\n",
    "\n",
    "eval_result_2 = tfma.run_model_analysis(\n",
    "    eval_shared_model=eval_shared_model_2,\n",
    "    eval_config=eval_config,\n",
    "    data_location=_EVAL_DATA_FILE,\n",
    "    output_path=\"./eval_result_150_steps\",\n",
    "    file_format='tfrecords',\n",
    "    slice_spec = slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfma.view.render_slicing_metrics(eval_result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results_from_disk = tfma.load_eval_results(\n",
    "    ['./eval_result_2000_steps','./eval_result_150_steps'], tfma.constants.MODEL_CENTRIC_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfma.view.render_time_series(eval_results_from_disk, slices[0])\n",
    "# many js errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating against thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config_threshold=tfma.EvalConfig(\n",
    "    model_specs=[tfma.ModelSpec(label_key='consumer_disputed')],\n",
    "    slicing_specs=[tfma.SlicingSpec(), tfma.SlicingSpec(feature_keys=['product'])],\n",
    "    metrics_specs=[\n",
    "          tfma.MetricsSpec(metrics=[\n",
    "              tfma.MetricConfig(class_name='BinaryAccuracy'),\n",
    "              tfma.MetricConfig(class_name='ExampleCount'),\n",
    "              tfma.MetricConfig(class_name='AUC')\n",
    "              ],\n",
    "              thresholds={\n",
    "                  'AUC':\n",
    "                      tfma.config.MetricThreshold(\n",
    "                          value_threshold=tfma.GenericValueThreshold(\n",
    "                              lower_bound={'value': 0.5}))}\n",
    "                          )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_shared_models = [\n",
    "  tfma.default_eval_shared_model(\n",
    "      model_name='candidate', # must have this exact name\n",
    "      eval_saved_model_path='serving_model_dir_150_steps/', tags=[tf.saved_model.SERVING]),\n",
    "  tfma.default_eval_shared_model(\n",
    "      model_name='baseline', # must have this exact name\n",
    "      eval_saved_model_path='serving_model_dir_2000_steps/', tags=[tf.saved_model.SERVING]),\n",
    "]\n",
    "\n",
    "eval_result = tfma.run_model_analysis(\n",
    "    eval_shared_models,\n",
    "    eval_config=eval_config_threshold,\n",
    "    data_location=_EVAL_DATA_FILE,\n",
    "    output_path=\"./eval_threshold\",slice_spec = slices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfma.load_validation_result('./eval_threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfma.view.render_slicing_metrics(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/tensorboard/blob/master/docs/fairness-indicators.md\n",
    "# needs environment without WIT,but with TF2.1, TFX\n",
    "!pip install tensorboard_plugin_fairness_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config_fairness=tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key='consumer_disputed')],\n",
    "        slicing_specs=[tfma.SlicingSpec(), tfma.SlicingSpec(feature_keys=['product'])],\n",
    "        metrics_specs=[\n",
    "              tfma.MetricsSpec(metrics=[\n",
    "                  tfma.MetricConfig(class_name='BinaryAccuracy'),\n",
    "                  tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                  tfma.MetricConfig(class_name='FalsePositives'),\n",
    "                  tfma.MetricConfig(class_name='TruePositives'),\n",
    "                  tfma.MetricConfig(class_name='FalseNegatives'),\n",
    "                  tfma.MetricConfig(class_name='TrueNegatives'),\n",
    "                  tfma.MetricConfig(class_name='FairnessIndicators', config='{\"thresholds\":[0.25, 0.5, 0.75]}')\n",
    "              ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Tensorflow version (2.1.0) found. Note that TFMA support for TF 2.0 is currently in beta\n",
      "WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "WARNING:absl:inputs do not match those expected by the model: input_names=['examples'], found in extracts={}\n",
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "WARNING:root:Deleting 1 existing files in target path matching: \n"
     ]
    }
   ],
   "source": [
    "eval_result = tfma.run_model_analysis(\n",
    "    eval_shared_model=eval_shared_model,\n",
    "    eval_config=eval_config_fairness,\n",
    "    data_location=_EVAL_DATA_FILE,\n",
    "    output_path=\"./eval_result_fairness\",\n",
    "    file_format='tfrecords',\n",
    "    slice_spec = slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard_plugin_fairness_indicators import summary_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.create_file_writer('./fairness_indicator_logs')\n",
    "with writer.as_default():\n",
    "    summary_v2.FairnessIndicators('./eval_result_fairness', step=1)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d90c50222392f6f9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d90c50222392f6f9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6007;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./fairness_indicator_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The What-If Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from witwidget.notebook.visualization import WitConfigBuilder\n",
    "from witwidget.notebook.visualization import WitWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = tf.data.TFRecordDataset(_EVAL_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_examples = [tf.train.Example.FromString(d.numpy()) for d in eval_data.take(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(export_dir=_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(examples):\n",
    "    preds = model.signatures['serving_default'](examples=tf.constant([example.SerializeToString() for example in examples]))\n",
    "    return preds['outputs'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_builder = WitConfigBuilder(eval_examples).set_custom_predict_fn(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a703a274a78943efac8e2cdec7b638c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "WitWidget(config={'model_type': 'classification', 'label_vocab': [], 'are_sequence_examples': False, 'inferenc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WitWidget(config_builder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works with 2.1\n",
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works with >0.21.3\n",
    "!pip show tensorflow_model_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works with 1.6.0\n",
    "!pip show witwidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing /Users/i854694/.virtualenvs/pipelines/lib/python3.7/site-packages/witwidget/static -> wit-widget\n",
      "- Validating: \u001b[32mOK\u001b[0m\n",
      "\n",
      "    To initialize this nbextension in the browser every time the notebook (or other app) loads:\n",
      "    \n",
      "          jupyter nbextension enable witwidget --py --sys-prefix\n",
      "    \n",
      "Enabling notebook extension wit-widget/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# may need to run this every time\n",
    "!jupyter nbextension install --py --symlink --sys-prefix witwidget\n",
    "\n",
    "!jupyter nbextension enable witwidget --py --sys-prefix \n",
    "\n",
    "# then refresh browser page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may need to run this every time\n",
    "\n",
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "  \n",
    "!jupyter nbextension install --py --symlink tensorflow_model_analysis --sys-prefix\n",
    "  \n",
    "!jupyter nbextension enable --py tensorflow_model_analysis --sys-prefix\n",
    "\n",
    "# then refresh browser page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyter_nbextensions_configurator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter serverextension list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
